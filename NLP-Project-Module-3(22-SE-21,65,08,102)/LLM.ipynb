{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "0MrzUiLykion",
        "outputId": "2a80737d-d681-4de5-d96c-b4c3a0f9b6f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-db14fcdf35be>:19: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  labels = labels_raw.applymap(lambda x: 1 if str(x).strip().lower() in ['1', 'true', 'yes'] else 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Loss: 0.5765\n",
            "Epoch 2/200, Loss: 0.5011\n",
            "Epoch 3/200, Loss: 0.4551\n",
            "Epoch 4/200, Loss: 0.4296\n",
            "Epoch 5/200, Loss: 0.4145\n",
            "Epoch 6/200, Loss: 0.4037\n",
            "Epoch 7/200, Loss: 0.3950\n",
            "Epoch 8/200, Loss: 0.3890\n",
            "Epoch 9/200, Loss: 0.3824\n",
            "Epoch 10/200, Loss: 0.3791\n",
            "Epoch 11/200, Loss: 0.3743\n",
            "Epoch 12/200, Loss: 0.3721\n",
            "Epoch 13/200, Loss: 0.3691\n",
            "Epoch 14/200, Loss: 0.3663\n",
            "Epoch 15/200, Loss: 0.3645\n",
            "Epoch 16/200, Loss: 0.3618\n",
            "Epoch 17/200, Loss: 0.3595\n",
            "Epoch 18/200, Loss: 0.3586\n",
            "Epoch 19/200, Loss: 0.3566\n",
            "Epoch 20/200, Loss: 0.3545\n",
            "Epoch 21/200, Loss: 0.3523\n",
            "Epoch 22/200, Loss: 0.3498\n",
            "Epoch 23/200, Loss: 0.3497\n",
            "Epoch 24/200, Loss: 0.3473\n",
            "Epoch 25/200, Loss: 0.3471\n",
            "Epoch 26/200, Loss: 0.3457\n",
            "Epoch 27/200, Loss: 0.3443\n",
            "Epoch 28/200, Loss: 0.3426\n",
            "Epoch 29/200, Loss: 0.3416\n",
            "Epoch 30/200, Loss: 0.3388\n",
            "Epoch 31/200, Loss: 0.3375\n",
            "Epoch 32/200, Loss: 0.3371\n",
            "Epoch 33/200, Loss: 0.3347\n",
            "Epoch 34/200, Loss: 0.3350\n",
            "Epoch 35/200, Loss: 0.3330\n",
            "Epoch 36/200, Loss: 0.3305\n",
            "Epoch 37/200, Loss: 0.3306\n",
            "Epoch 38/200, Loss: 0.3298\n",
            "Epoch 39/200, Loss: 0.3271\n",
            "Epoch 40/200, Loss: 0.3264\n",
            "Epoch 41/200, Loss: 0.3258\n",
            "Epoch 42/200, Loss: 0.3239\n",
            "Epoch 43/200, Loss: 0.3240\n",
            "Epoch 44/200, Loss: 0.3221\n",
            "Epoch 45/200, Loss: 0.3206\n",
            "Epoch 46/200, Loss: 0.3194\n",
            "Epoch 47/200, Loss: 0.3200\n",
            "Epoch 48/200, Loss: 0.3172\n",
            "Epoch 49/200, Loss: 0.3162\n",
            "Epoch 50/200, Loss: 0.3153\n",
            "Epoch 51/200, Loss: 0.3143\n",
            "Epoch 52/200, Loss: 0.3126\n",
            "Epoch 53/200, Loss: 0.3128\n",
            "Epoch 54/200, Loss: 0.3108\n",
            "Epoch 55/200, Loss: 0.3105\n",
            "Epoch 56/200, Loss: 0.3084\n",
            "Epoch 57/200, Loss: 0.3077\n",
            "Epoch 58/200, Loss: 0.3067\n",
            "Epoch 59/200, Loss: 0.3062\n",
            "Epoch 60/200, Loss: 0.3051\n",
            "Epoch 61/200, Loss: 0.3040\n",
            "Epoch 62/200, Loss: 0.3034\n",
            "Epoch 63/200, Loss: 0.3020\n",
            "Epoch 64/200, Loss: 0.3009\n",
            "Epoch 65/200, Loss: 0.3003\n",
            "Epoch 66/200, Loss: 0.2979\n",
            "Epoch 67/200, Loss: 0.2974\n",
            "Epoch 68/200, Loss: 0.2963\n",
            "Epoch 69/200, Loss: 0.2962\n",
            "Epoch 70/200, Loss: 0.2949\n",
            "Epoch 71/200, Loss: 0.2918\n",
            "Epoch 72/200, Loss: 0.2937\n",
            "Epoch 73/200, Loss: 0.2908\n",
            "Epoch 74/200, Loss: 0.2909\n",
            "Epoch 75/200, Loss: 0.2899\n",
            "Epoch 76/200, Loss: 0.2878\n",
            "Epoch 77/200, Loss: 0.2870\n",
            "Epoch 78/200, Loss: 0.2866\n",
            "Epoch 79/200, Loss: 0.2855\n",
            "Epoch 80/200, Loss: 0.2838\n",
            "Epoch 81/200, Loss: 0.2838\n",
            "Epoch 82/200, Loss: 0.2827\n",
            "Epoch 83/200, Loss: 0.2808\n",
            "Epoch 84/200, Loss: 0.2777\n",
            "Epoch 85/200, Loss: 0.2799\n",
            "Epoch 86/200, Loss: 0.2776\n",
            "Epoch 87/200, Loss: 0.2771\n",
            "Epoch 88/200, Loss: 0.2768\n",
            "Epoch 89/200, Loss: 0.2767\n",
            "Epoch 90/200, Loss: 0.2747\n",
            "Epoch 91/200, Loss: 0.2731\n",
            "Epoch 92/200, Loss: 0.2710\n",
            "Epoch 93/200, Loss: 0.2716\n",
            "Epoch 94/200, Loss: 0.2698\n",
            "Epoch 95/200, Loss: 0.2697\n",
            "Epoch 96/200, Loss: 0.2676\n",
            "Epoch 97/200, Loss: 0.2672\n",
            "Epoch 98/200, Loss: 0.2659\n",
            "Epoch 99/200, Loss: 0.2654\n",
            "Epoch 100/200, Loss: 0.2650\n",
            "Epoch 101/200, Loss: 0.2628\n",
            "Epoch 102/200, Loss: 0.2629\n",
            "Epoch 103/200, Loss: 0.2608\n",
            "Epoch 104/200, Loss: 0.2595\n",
            "Epoch 105/200, Loss: 0.2587\n",
            "Epoch 106/200, Loss: 0.2580\n",
            "Epoch 107/200, Loss: 0.2560\n",
            "Epoch 108/200, Loss: 0.2559\n",
            "Epoch 109/200, Loss: 0.2537\n",
            "Epoch 110/200, Loss: 0.2534\n",
            "Epoch 111/200, Loss: 0.2541\n",
            "Epoch 112/200, Loss: 0.2509\n",
            "Epoch 113/200, Loss: 0.2506\n",
            "Epoch 114/200, Loss: 0.2504\n",
            "Epoch 115/200, Loss: 0.2488\n",
            "Epoch 116/200, Loss: 0.2473\n",
            "Epoch 117/200, Loss: 0.2458\n",
            "Epoch 118/200, Loss: 0.2441\n",
            "Epoch 119/200, Loss: 0.2437\n",
            "Epoch 120/200, Loss: 0.2424\n",
            "Epoch 121/200, Loss: 0.2415\n",
            "Epoch 122/200, Loss: 0.2421\n",
            "Epoch 123/200, Loss: 0.2394\n",
            "Epoch 124/200, Loss: 0.2390\n",
            "Epoch 125/200, Loss: 0.2359\n",
            "Epoch 126/200, Loss: 0.2362\n",
            "Epoch 127/200, Loss: 0.2358\n",
            "Epoch 128/200, Loss: 0.2344\n",
            "Epoch 129/200, Loss: 0.2335\n",
            "Epoch 130/200, Loss: 0.2330\n",
            "Epoch 131/200, Loss: 0.2328\n",
            "Epoch 132/200, Loss: 0.2303\n",
            "Epoch 133/200, Loss: 0.2295\n",
            "Epoch 134/200, Loss: 0.2278\n",
            "Epoch 135/200, Loss: 0.2278\n",
            "Epoch 136/200, Loss: 0.2251\n",
            "Epoch 137/200, Loss: 0.2249\n",
            "Epoch 138/200, Loss: 0.2252\n",
            "Epoch 139/200, Loss: 0.2217\n",
            "Epoch 140/200, Loss: 0.2214\n",
            "Epoch 141/200, Loss: 0.2208\n",
            "Epoch 142/200, Loss: 0.2198\n",
            "Epoch 143/200, Loss: 0.2189\n",
            "Epoch 144/200, Loss: 0.2195\n",
            "Epoch 145/200, Loss: 0.2163\n",
            "Epoch 146/200, Loss: 0.2152\n",
            "Epoch 147/200, Loss: 0.2148\n",
            "Epoch 148/200, Loss: 0.2123\n",
            "Epoch 149/200, Loss: 0.2121\n",
            "Epoch 150/200, Loss: 0.2101\n",
            "Epoch 151/200, Loss: 0.2097\n",
            "Epoch 152/200, Loss: 0.2096\n",
            "Epoch 153/200, Loss: 0.2069\n",
            "Epoch 154/200, Loss: 0.2062\n",
            "Epoch 155/200, Loss: 0.2059\n",
            "Epoch 156/200, Loss: 0.2038\n",
            "Epoch 157/200, Loss: 0.2037\n",
            "Epoch 158/200, Loss: 0.2033\n",
            "Epoch 159/200, Loss: 0.2003\n",
            "Epoch 160/200, Loss: 0.2013\n",
            "Epoch 161/200, Loss: 0.1985\n",
            "Epoch 162/200, Loss: 0.1992\n",
            "Epoch 163/200, Loss: 0.1979\n",
            "Epoch 164/200, Loss: 0.1970\n",
            "Epoch 165/200, Loss: 0.1946\n",
            "Epoch 166/200, Loss: 0.1944\n",
            "Epoch 167/200, Loss: 0.1936\n",
            "Epoch 168/200, Loss: 0.1928\n",
            "Epoch 169/200, Loss: 0.1902\n",
            "Epoch 170/200, Loss: 0.1886\n",
            "Epoch 171/200, Loss: 0.1895\n",
            "Epoch 172/200, Loss: 0.1874\n",
            "Epoch 173/200, Loss: 0.1864\n",
            "Epoch 174/200, Loss: 0.1849\n",
            "Epoch 175/200, Loss: 0.1848\n",
            "Epoch 176/200, Loss: 0.1837\n",
            "Epoch 177/200, Loss: 0.1832\n",
            "Epoch 178/200, Loss: 0.1829\n",
            "Epoch 179/200, Loss: 0.1802\n",
            "Epoch 180/200, Loss: 0.1792\n",
            "Epoch 181/200, Loss: 0.1790\n",
            "Epoch 182/200, Loss: 0.1771\n",
            "Epoch 183/200, Loss: 0.1750\n",
            "Epoch 184/200, Loss: 0.1763\n",
            "Epoch 185/200, Loss: 0.1747\n",
            "Epoch 186/200, Loss: 0.1735\n",
            "Epoch 187/200, Loss: 0.1712\n",
            "Epoch 188/200, Loss: 0.1729\n",
            "Epoch 189/200, Loss: 0.1732\n",
            "Epoch 190/200, Loss: 0.1696\n",
            "Epoch 191/200, Loss: 0.1695\n",
            "Epoch 192/200, Loss: 0.1657\n",
            "Epoch 193/200, Loss: 0.1668\n",
            "Epoch 194/200, Loss: 0.1671\n",
            "Epoch 195/200, Loss: 0.1645\n",
            "Epoch 196/200, Loss: 0.1621\n",
            "Epoch 197/200, Loss: 0.1612\n",
            "Epoch 198/200, Loss: 0.1614\n",
            "Epoch 199/200, Loss: 0.1592\n",
            "Epoch 200/200, Loss: 0.1590\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Training complete ✅'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"/content/final_training_dataset.csv\")\n",
        "\n",
        "# Extract inputs and labels\n",
        "texts = df['post'].astype(str).tolist()\n",
        "label_cols = df.columns[1:]\n",
        "labels_raw = df[label_cols]\n",
        "\n",
        "# Convert string labels (if necessary) to binary 0/1\n",
        "labels = labels_raw.applymap(lambda x: 1 if str(x).strip().lower() in ['1', 'true', 'yes'] else 0)\n",
        "labels_tensor = torch.tensor(labels.values).float()\n",
        "\n",
        "# Load modern embedding model (BGE)\n",
        "embed_model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
        "embeddings = embed_model.encode(texts, batch_size=32, convert_to_tensor=True)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    embeddings, labels_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Define the classifier\n",
        "class DepressionClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=9):\n",
        "        super(DepressionClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(512, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        return self.sigmoid(self.fc2(x))\n",
        "\n",
        "# Model, loss, optimizer\n",
        "model = DepressionClassifier(input_dim=embeddings.shape[1])\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "EPOCHS = 200\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "trained_model = model  # for inference later\n",
        "\"Training complete ✅\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Load test dataset\n",
        "test_df = pd.read_csv(\"/content/final_testing_dataset.csv\")\n",
        "\n",
        "# Extract post texts\n",
        "test_texts = test_df['post'].astype(str).tolist()\n",
        "\n",
        "# Encode posts using the same SentenceTransformer model\n",
        "test_embeddings = embed_model.encode(test_texts, batch_size=32, convert_to_tensor=True).to(device)\n",
        "\n",
        "# Put model in eval mode\n",
        "trained_model.eval()\n",
        "\n",
        "# Predict for each embedding\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    outputs = trained_model(test_embeddings)\n",
        "    predictions = (outputs > 0.5).int().cpu().numpy()\n",
        "\n",
        "# Convert predictions to DataFrame\n",
        "predicted_labels_df = pd.DataFrame(predictions, columns=label_cols)\n",
        "\n",
        "# Combine with original test_df (optional)\n",
        "final_df = pd.concat([test_df, predicted_labels_df], axis=1)\n",
        "\n",
        "# Save results (optional)\n",
        "final_df.to_csv(\"/content/test_predictions.csv\", index=False)\n",
        "\n",
        "# Preview results\n",
        "print(final_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OerwvIuGnm9u",
        "outputId": "d1abfc1b-dbb8-4f4f-8d8b-b9aa6b8f52e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0                                               post  anger  \\\n",
            "0           0  I feel unloved, I feel like a burden to everyo...      0   \n",
            "1           1  my grandfather had mental health problem, two ...      0   \n",
            "2           2  The older I get the more and more I feel isola...      0   \n",
            "3           3  I can’t handle poverty any more. Everything is...      1   \n",
            "4           4  This my first Valentine’s Day post separation ...      0   \n",
            "\n",
            "   brain dysfunction (forget)  emptiness  hopelessness  loneliness  sadness  \\\n",
            "0                           0          1             1           1        1   \n",
            "1                           0          0             1           0        1   \n",
            "2                           0          1             1           1        1   \n",
            "3                           0          1             1           0        1   \n",
            "4                           0          1             0           1        0   \n",
            "\n",
            "   suicide intent  worthlessness  post  anger  brain dysfunction (forget)  \\\n",
            "0               1              1     0      1                           0   \n",
            "1               1              0     0      0                           0   \n",
            "2               0              1     0      0                           0   \n",
            "3               1              1     0      0                           0   \n",
            "4               0              0     0      0                           0   \n",
            "\n",
            "   emptiness  hopelessness  loneliness  sadness  suicide intent  worthlessness  \n",
            "0          1             1           1        1               1              1  \n",
            "1          0             1           0        1               1              0  \n",
            "2          1             1           1        1               0              0  \n",
            "3          1             1           1        1               0              1  \n",
            "4          1             0           1        1               0              0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load true labels from test.csv (if available)\n",
        "# Assuming your test.csv contains ground-truth labels\n",
        "true_labels = test_df[label_cols].applymap(lambda x: 1 if str(x).strip().lower() in ['1', 'true', 'yes'] else 0).values\n",
        "\n",
        "# Predict using trained model\n",
        "trained_model.eval()\n",
        "with torch.no_grad():\n",
        "    test_embeddings = embed_model.encode(test_df['post'].astype(str).tolist(), batch_size=32, convert_to_tensor=True).to(device)\n",
        "    outputs = trained_model(test_embeddings)\n",
        "    predictions = (outputs > 0.5).int().cpu().numpy()\n",
        "\n",
        "# Compute metrics\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "f1_micro = f1_score(true_labels, predictions, average='micro')\n",
        "f1_macro = f1_score(true_labels, predictions, average='macro')\n",
        "precision = precision_score(true_labels, predictions, average='micro')\n",
        "recall = recall_score(true_labels, predictions, average='micro')\n",
        "\n",
        "# Print results\n",
        "print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
        "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzE23q-yo-ne",
        "outputId": "240af8a2-52b3-4552-ab19-1264d385c77c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-20fc3f7f38da>:5: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  true_labels = test_df[label_cols].applymap(lambda x: 1 if str(x).strip().lower() in ['1', 'true', 'yes'] else 0).values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (Micro): 0.7563\n",
            "F1 Score (Macro): 0.6219\n",
            "Precision: 0.7789\n",
            "Recall: 0.7350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}